# LocalAgenticRAG_QdrantAgno
We are building a Local Agentic RAG using Qdrant and Agno . 

<<<<<<< HEAD
------------------------------------------------------------------------------------

Architecture : https://excalidraw.com/#json=GD-N6SZikgzoaw1pirf3Z,02HDcXNSOOVSTERXYVZkOg

Learn how to build a Retrieval-Augmented Generation (RAG) system to chat with your data using Langchain and Agno (formerly known as Phidata) completely locally, without relying on OpenAI or Gemini API keys.

In this step-by-step guide, you'll discover how to:

Set up a local RAG pipeline i.e., Chat with Website for enhanced data privacy and control.
Utilize Langchain and Agno to orchestrate your Agentic RAG.
Implement Qdrant for vector storage and retrieval.
Generate embeddings locally with FastEmbed (by Qdrant) for lightweight-fast performance.
Run Large Language Models (LLMs) locally using Ollama. [might be slow based on device]

-----------------------------------------------------------------------------------


=======
![Image Alt](https://github.com/SetuAI/LocalAgenticRAG_QdrantAgno/blob/11f5e35d8031efeeb0e9d35e05e619ebea94a3a0/LocalAgenticRAG.png)
>>>>>>> 11677606e5be8f69d8bf90d50f1f83f7c9c8e9c3


inference.py : Works with FastEmbed by Qdrant
inference2.py : works with qwen2.5:0b and mxbai embedding model.